---
title: 离散选择模型（一）：基础知识（Discrete Choice Model, DCM）学习笔记
date: 2020-03-31
permalink: /data-mining-dcm-one.html
tags:
 - 数据挖掘
 - 离散选择模型
categories:
 - 数据挖掘

---





## 起因

当因变量是分类变量时，线性回归就会存在一定的局限性，主要是因为分类变量违背了线性回归的部分假设条件，就会导致得出的参数值是有偏的。
线性回归模型成立的假设条件：

![](https://img-blog.csdnimg.cn/20200331090747128.png)

  1. 自变量X每增加一个单位，对Y的影响都是一样的。
  2. 误差项的期望值为0，即误差项和自变量不相关
  3. 误差项的方差相同
  4. 误差项是一个服从正态分布的随机变量

对于分类变量，因变量只能取几个固定的值，误差项与自变量无关，误差项也只能取对应的固定值，所以误差项无法满足正态分布。

## 要素

![](https://img-blog.csdnimg.cn/20200331170211282.png)

## 类型

![](https://img-blog.csdnimg.cn/20200331170251160.png)

## Logit模型

  1. 对于分类变量，我们用某一类别发生的概率作为因变量
  2. 则此因变量的取值范围为[0, 1]，无法应用到线性回归模型中
  3. `概率`定义为事件出现的次数与所有结果出现的次数之比，`Odds`定义为事件发生的概率与事件不发生的概率之比，$Odds=P / (1-P)$
  4. Odds的取值范围为$[0, +\infty]$，此时对Odds去自然对数，则可以将取值范围扩大到$[-\infty, +\infty]$，从而满足线性回归因变量的取值范围。
  5. 由此，得到二项Logit模型的基本形式： $$\log i t\left(P_{i}\right)=\ln \frac{P_{i}}{1-P_{i}}=\beta_{0}+\beta_{1} x_{1, i}+\beta_{2} x_{2, i}+\cdots+\beta_{n} x_{n, i}$$

## 系数解读

从胜率（Odds）角度 以系数$\beta_1$为例，如果$x_1$是连续变量，当$x_1$变化1个单位且其他变量保持不变时，新的Logit'变为：
$$\begin{array}{l} \log i t^{\prime}=\ln \left(\frac{\not
p_{1}}{p_{0}}\right)=\beta_{0}+\beta_{1}\left(x_{1}+1\right)+\beta_{2}
x_{2}+\cdots+\beta_{n} x_{n} \ =\beta_{0}+\beta_{1} x_{1}+\beta_{2}
x_{2}+\cdots+\beta_{n} x_{n}+\beta_{1}=\log i t+\beta_{1} \end{array}$$
对上式两边取自然对数，即$x_1$变化1个单位时，胜率Odds变成了原来的$e^{\beta_1}$倍。
$$\left(\frac{p_{1}}{p_{0}}\right)^{\prime}=\left(\frac{p_{1}}{p_{0}}\right)
\cdot e^{\beta_{1}}$$
若$x_1$为分类变量，则系数$\beta_1$可以解读为：其它变量保持不变时，分类变量的取值从参照类（reference
category）变化到当前类时，胜率变成原来的$e^{\beta_1}$倍。

## 与LR的比较

![](https://img-blog.csdnimg.cn/20200331170549200.png)

## Logit和Logistic

其实是一样的，可以相互转化。 当我们说Logit模型的时候，一般指的就是这个式子： $$\log
\left(\frac{P}{1-P}\right)=\beta_{0}+\beta_{1} x_{1}$$
当我们说Logistic模型的时候，一般指的是这个式子： $$P=\frac{1}{1+e^{-\left(\beta_{0}+\beta_{1}
x_{1}\right)}}$$

## 共线性问题

  1. 定义 多重共线性：自变量之间存在较高的线性相关度。
  2. 原因 

    * 解释模型的系数时，需要强调 **其他变量保持不变** ，当变量之间存在较高的线性相关度时，这个条件便无法满足，即变量之间应该是相互独立的。
    * 导致参数估计不准
    * 导致自变量的显著性判断出错
    * 导致参数估计值方差增大，模型参数不稳定
  3. 判断 

    * 相关系数矩阵(correlation matrix)：只能判断两个变量之间是否线性相关
    * 方差膨胀因子(Variance Inflation Factor, VIF) 比如，我们要拟合的模型为： $$y=\beta_{0}+\beta_{1} x_{1}+\beta_{2} x_{2}+\beta_{3} x_{3}+\beta_{4} x_{4}+\varepsilon$$ 为了计算$x_1$的VIF值，我们就以$x_1$为因变量， $x_2$ 、$x_3$、$x_4$为自变量，拟合一个新的模型： $$x_{1}=\beta_{0}^{*}+\beta_{1}^{*} x_{2}+\beta_{2}^{*} x_{3}+\beta_{2}^{*} x_{4}+\varepsilon^{*}$$ 记上述模型的拟合优度为$R^2$ ，则$x_1$的VIF值为： $$V I F=\frac{1}{1-R^{2}}$$ 实际意义：变量$x_1$的VIF值等于1.8表示的是：当变量$x_1$与其它自变量之间存在相关性时，其参数估计值的方差是没有相关性时的1.8倍（膨胀了80%）。VIF值越大，多重共线性越严重。一般认为VIF大于10时（严格是5），模型存在严重的共线性问题。
  4. 解决 

    * 存在共线性，但是VIF值不是很大，可以忽略
    * 共线性会导致系数估计方差变大，但是模型预测能力不会降低。如果只是预测，而不在乎模型的可解释的话，可以忽略
    * 删除变量，比如模型中变量A和B高度相关，移除B以后，模型的预测效果并没有显著降低，此时可以考虑移除变量B
    * 定义一个新的变量来替代原来模型中的相关变量。继续上面的假设——变量A和B高度相关，可以定义C=A+B、或者C=A*B放入模型中
    * 使用专门处理变量之间相关性的模型，比如PCA就是通过正交变换将一组可能存在相关性的变量转换成一组线性不相关的变量。

## 效用最大化准则

影响决策的是方案$i$的效用和方案$j$的效用的相对差值

## 参考书

![](https://img-blog.csdnimg.cn/20200331113258102.png)

> 参考 https://zhuanlan.zhihu.com/logit

