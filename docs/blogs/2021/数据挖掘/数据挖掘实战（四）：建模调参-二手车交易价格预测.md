---
title: 数据挖掘实战（四）：建模调参-二手车交易价格预测
date: 2020-04-01
permalink: /data-mining-practice-four.html
tags:
 - 数据挖掘
 - 二手车价格预测 
categories:
 - 数据挖掘

---



## 建立模型

### 读取数据


​    
    import pandas as pd
    import numpy as np
    import warnings
    warnings.filterwarnings('ignore')


reduce_mem_usage 函数通过调整数据类型，帮助我们减少数据在内存中占用的空间


​    
    def reduce_mem_usage(df):
        """ iterate through all the columns of a dataframe and modify the data type
            to reduce memory usage.        
        """
        start_mem = df.memory_usage().sum() 
        print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))
    
        for col in df.columns:
            col_type = df[col].dtype
    
            if col_type != object:
                c_min = df[col].min()
                c_max = df[col].max()
                if str(col_type)[:3] == 'int':
                    if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:
                        df[col] = df[col].astype(np.int8)
                    elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:
                        df[col] = df[col].astype(np.int16)
                    elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:
                        df[col] = df[col].astype(np.int32)
                    elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:
                        df[col] = df[col].astype(np.int64)  
                else:
                    if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:
                        df[col] = df[col].astype(np.float16)
                    elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:
                        df[col] = df[col].astype(np.float32)
                    else:
                        df[col] = df[col].astype(np.float64)
            else:
                df[col] = df[col].astype('category')
    
        end_mem = df.memory_usage().sum() 
        print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))
        print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) / start_mem))
        return df


​    
​    
    sample_feature = reduce_mem_usage(pd.read_csv('data_for_tree.csv'))


Memory usage of dataframe is 58915080.00 MB Memory usage after optimization
is: 16321162.00 MB Decreased by 72.3%
查看各个特征的类型，发现notRepairedDamage特征的类型由object转换为float16


​    
    sample_feature.info()


​    
​    
    continuous_feature_names = [x for x in sample_feature.columns if x not in ['price','brand','model']]


筛选出连续变量，用于线性回归

### 简单线性回归

dropna函数删除了含有nan值的记录，由下可见删除了有9万条左右，reset_index重新设置索引。


​    
    sample_feature = sample_feature.dropna().replace('-', 0).reset_index(drop=True)


​    
​    
    from sklearn.linear_model import LinearRegression
    model = LinearRegression(normalize=True)
    model = model.fit(train_X, train_y)
    'intercept:' + str(model.intercept_)
    sorted(dict(zip(continuous_feature_names, model.coef_)).items(), key=lambda x:x[1], reverse=True)


sorted函数排序，第二个参数key是用来进行比较的元素，其中x取自前面的字典item，x[1]表示特征对应的系数，所以意思是按照特征系数的大小的名称降序排列。
对标签进行log(x+1)的变换，使其贴近于正态分布，之所以+1，是为了让price全部为正值。


​    
    train_y_ln = np.log(train_y + 1) 


### 五折交叉验证

数据集一般分为：训练集（train_set），评估集（valid_set），测试集（test_set）这三个部分。因此我们通常并不会把所有的数据集都拿来训练，而是分出一部分来（这部分就是好评估集）对训练集生成的参数进行测试，相对客观的判断这些参数对训练集之外的数据的符合程度。这种思想就称为交叉验证（Cross
Validation）


​    
    from sklearn.model_selection import cross_val_score
    from sklearn.metrics import mean_absolute_error, make_scorer


​    
​    
    def log_transfer(func):
        def wrapper(y, yhat):
            result = func(np.log(y), np.nan_to_num(np.log(yhat)))
            return result
        return wrapper


​    
​    
    scores = cross_val_score(model, X=train_X, y=train_y, verbose=1, cv = 5, 
                             scoring=make_scorer(log_transfer(mean_absolute_error)))


​    
​    
    scores = cross_val_score(model, X=train_X, y=train_y_ln, verbose=1, cv = 5, scoring=make_scorer(mean_absolute_error))


## 模型调参

以LGB为例 网格调参和贪心调参比较好理解，网格调参十分耗时，贪心调参容易陷入局部最优，贝叶斯调整速度快效果好。


​    
    ## LGB的参数集合：
    
    objective = ['regression', 'regression_l1', 'mape', 'huber', 'fair']
    
    num_leaves = [3,5,10,15,20,40, 55]
    max_depth = [3,5,10,15,20,40, 55]
    bagging_fraction = []
    feature_fraction = []
    drop_rate = []


### Grid Search调参


​    
    from sklearn.model_selection import GridSearchCV
    parameters = {'objective': objective , 'num_leaves': num_leaves, 'max_depth': max_depth}
    model = LGBMRegressor()
    clf = GridSearchCV(model, parameters, cv=5)
    clf = clf.fit(train_X, train_y)
    clf.best_params_
    model = LGBMRegressor(objective='regression',
                              num_leaves=55,
                              max_depth=15)
    np.mean(cross_val_score(model, X=train_X, y=train_y_ln, verbose=0, cv = 5, scoring=make_scorer(mean_absolute_error)))


### 贪心调参


​    
    best_obj = dict()
    for obj in objective:
        model = LGBMRegressor(objective=obj)
        score = np.mean(cross_val_score(model, X=train_X, y=train_y_ln, verbose=0, cv = 5, scoring=make_scorer(mean_absolute_error)))
        best_obj[obj] = score
    
    best_leaves = dict()
    for leaves in num_leaves:
        model = LGBMRegressor(objective=min(best_obj.items(), key=lambda x:x[1])[0], num_leaves=leaves)
        score = np.mean(cross_val_score(model, X=train_X, y=train_y_ln, verbose=0, cv = 5, scoring=make_scorer(mean_absolute_error)))
        best_leaves[leaves] = score
    
    best_depth = dict()
    for depth in max_depth:
        model = LGBMRegressor(objective=min(best_obj.items(), key=lambda x:x[1])[0],
                              num_leaves=min(best_leaves.items(), key=lambda x:x[1])[0],
                              max_depth=depth)
        score = np.mean(cross_val_score(model, X=train_X, y=train_y_ln, verbose=0, cv = 5, scoring=make_scorer(mean_absolute_error)))
        best_depth[depth] = score


### 贝叶斯调参

安装`pip install bayesian-optimization`


​    
    from bayes_opt import BayesianOptimization
    
    def rf_cv(num_leaves, max_depth, subsample, min_child_samples):
        val = cross_val_score(
            LGBMRegressor(objective = 'regression_l1',
                num_leaves=int(num_leaves),
                max_depth=int(max_depth),
                subsample = subsample,
                min_child_samples = int(min_child_samples)
            ),
            X=train_X, y=train_y_ln, verbose=0, cv = 5, scoring=make_scorer(mean_absolute_error)
        ).mean()
        return 1 - val
    rf_bo = BayesianOptimization(
        rf_cv,
        {
        'num_leaves': (2, 100),
        'max_depth': (2, 100),
        'subsample': (0.1, 1),
        'min_child_samples' : (2, 100)
        }
    )
    rf_bo.maximize()
    1 - rf_bo.max['target']

