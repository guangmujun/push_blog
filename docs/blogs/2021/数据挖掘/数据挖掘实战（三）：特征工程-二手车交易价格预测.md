---
title: 数据挖掘实战（三）：特征工程-二手车交易价格预测
date: 2020-03-27
permalink: /data-mining-practice-three.html
tags:
 - 数据挖掘
 - 二手车价格预测 
categories:
 - 数据挖掘
---



## 基本介绍

  1. 重要性 调参效果有限，特征工程的好坏决定最终的排名和成绩
  2. 目的 将数据转换为能更好地表示潜在问题的特征

## 内容介绍（精华）

说明：以下内容中，加粗的部分为实战中使用到的方法，有具体的实现代码，剩余的相关处理技术后续再补充上。 常见的特征工程包括： 1\. 异常处理： \- 通过
**箱线图** （或3-Sigma）分析删除异常值 \- BOX-COX转换（处理有偏分布） \- 长尾截断 2\. 特征归一化/标准化： \-
标准化（转换为标准正态分布） \- **归一化** （转换到[0, 1]区间） \- 针对幂律分布，可以采用公式：$\log
\left(\frac{1+x}{1+m e \operatorname{dian}}\right)$ 3\. 数据分桶： \- 等频分桶 \-
**等距分桶** \- Best-KS分桶（类似利用基尼指数进行二分类） \- 卡方分桶 4\. 缺失值处理： \- **不处理**
（针对类似XGBoost等树模型） \- 删除（特征缺失的数据太多，可以考虑删除） \-
插值补全，包括均值/中位数/众数/建模预测/多重插补/压缩感知补全/矩阵补全等 \- 分箱，缺失值一个箱 5\. 特征构造： \- **构造统计量特征**
，报告计数，求和，比例，标准差等 \- **时间特征** ，包括相对时间和绝对时间，节假日，双休日等 \- **地理信息** ，包括分箱，分布编码等方法
\- **非线性变换** ，包括log/平方/根号等 \- 特征组合，特征交叉 \- 仁者见仁，智者见智 6\. 特征筛选 \-
过滤式（filter）：先对数据进行特征选择，然后再训练学习器，常见的方法有Relief/方差选择法/相关系数法/卡方检验法/互信息法 \-
包裹式（wrapper）：直接把最终将要使用的学习器的性能作为特征子集的评价准则，常见方法有LVM（Las Vegas Wrapper） \-
**嵌入式** （embedding）：结果过滤式和包裹式，学习器训练过程中自动进行了特征选择，常见的有lasso回归 7\. 降维 \-
PCA/LDA/ICA

## 代码示例

### 导入数据


​    
    import pandas as pd
    import numpy as np
    import matplotlib
    import matplotlib.pyplot as plt
    import seaborn as sns
    
    Train_data = pd.read_csv('used_car_train_20200313.csv', sep=' ')
    Test_data = pd.read_csv('used_car_testA_20200313.csv', sep=' ')
    print(Train_data.shape)
    print(Test_data.shape)


### 删除异常值

下面为利用 **箱线图** 剔除异常值的函数


​    
    def outliers_proc(data, col_name, scale=3):
        """
        用于清洗异常值，默认用 box_plot（scale=3）进行清洗
        :param data: 接收 pandas 数据格式
        :param col_name: pandas 列名
        :param scale: 尺度
        :return:
        """
    
        def box_plot_outliers(data_ser, box_scale):
            """
            利用箱线图去除异常值
            :param data_ser: 接收 pandas.Series 数据格式
            :param box_scale: 箱线图尺度，
            :return:
            """
            iqr = box_scale * (data_ser.quantile(0.75) - data_ser.quantile(0.25))
            val_low = data_ser.quantile(0.25) - iqr
            val_up = data_ser.quantile(0.75) + iqr
            rule_low = (data_ser < val_low)
            rule_up = (data_ser > val_up)
            return (rule_low, rule_up), (val_low, val_up)
    
        data_n = data.copy()
        data_series = data_n[col_name]
        rule, value = box_plot_outliers(data_series, box_scale=scale)
        index = np.arange(data_series.shape[0])[rule[0] | rule[1]]
        print("Delete number is: {}".format(len(index)))
        data_n = data_n.drop(index)
        data_n.reset_index(drop=True, inplace=True)
        print("Now column number is: {}".format(data_n.shape[0]))
        index_low = np.arange(data_series.shape[0])[rule[0]]
        outliers = data_series.iloc[index_low]
        print("Description of data less than the lower bound is:")
        print(pd.Series(outliers).describe())
        index_up = np.arange(data_series.shape[0])[rule[1]]
        outliers = data_series.iloc[index_up]
        print("Description of data larger than the upper bound is:")
        print(pd.Series(outliers).describe())
    
        fig, ax = plt.subplots(1, 2, figsize=(10, 7))
        sns.boxplot(y=data[col_name], data=data, palette="Set1", ax=ax[0])
        sns.boxplot(y=data_n[col_name], data=data_n, palette="Set1", ax=ax[1])
        return data_n


实战中，删除power特征的异常数据。 **注意：测试集的数据不能删除**


​    
    Train_data = outliers_proc(Train_data, 'power', scale=3)


运行结果：

![](https://img-blog.csdnimg.cn/20200325211540241.png)

![](https://img-blog.csdnimg.cn/20200325211658549.png)

### 特征构造

  1. 测试集和训练集放到一起，对特征进行处理。


​    
    Train_data['train']=1
    Test_data['train']=0
    data = pd.concat([Train_data, Test_data], ignore_index=True)  


  2. **时间特征** 处理 使用时间`data['creatDate'] - data['regDate']`反应汽车使用时间，一般来说价格与使用时间成反比。不过要注意，数据里有时间出错的格式，所以我们需要使用`errors='coerce'`将无效值强制转换为NaN。


​    
    data['used_time'] = (pd.to_datetime(data['creatDate'], format='%Y%m%d', errors='coerce') - 
                        pd.to_datetime(data['regDate'], format='%Y%m%d', errors='coerce')).dt.days
    
    data['used_time'].head()


  3. **缺失值处理** ，看一下空数据，有 约15k 个样本的时间是有问题的，我们可以选择删除，也可以选择放着。但是这里不建议删除，因为删除缺失数据占总样本量过大，7.5%。我们可以先放着，因为如果我们 XGBoost 之类的决策树，其本身就能处理缺失值，所以可以不用管；


​    
    data['used_time'].isnull().sum()


  4. **地理编码** 处理 从邮编中提取城市信息，相当于加入了先验知识。提取`regionCode`的第一位代表不同的城市。


​    
    data['city'] = data['regionCode'].apply(lambda x: str(x)[:-3])
    data = data
    data['city'].head()


  5. **构造统计量特征** 此处计算某品牌的销售统计量，也可以计算其他特征的统计量，这里要用train 的数据计算统计量。


​    
    Train_gb = Train_data.groupby('brand')  # 按找brand这一列分组
    all_info = {}  # 键：某一品牌，值：这一品牌车的统计信息
    for kind, kind_data in Train_gb:  # kind表示具体的一类，kind_data表示这一类中的所有数据
        info = {}
        kind_data = kind_data[kind_data['price'] > 0]  # 挑选出这一类中price>0的
        info['brand_amount'] = len(kind_data)  # 统计出这一品牌中车的总数
        info['brand_price_max'] = kind_data.price.max()  # 这一品牌的车种价格最高是多少
        info['brand_price_median'] = kind_data.price.median()
        info['brand_price_min'] = kind_data.price.min()
        info['brand_price_sum'] = kind_data.price.sum()
        info['brand_price_std'] = kind_data.price.std()
        info['brand_price_average'] = round(kind_data.price.sum() / (len(kind_data)+1), 2)
        all_info[kind] = info
    
    # 字典转变为DataFrame;还原索引，从新变为默认的整型索引;重命名类名
    brand_fe = pd.DataFrame(all_info).T.reset_index().rename(columns={'index':'brand'})
    brand_fe.head()


运行结果示例：

![](https://img-blog.csdnimg.cn/20200325213350909.png)

将上述分组统计的数据，合并到原来的数据集中


​    
    data = data.merge(brand_fe, how='left', on='brand')
    data.head()


![](https://img-blog.csdnimg.cn/2020032521352241.png)

![](https://img-blog.csdnimg.cn/20200325213547161.png)

### 数据分桶

  1. 数据分桶的原因 
        1. 离散后稀疏向量内积乘法运算速度更快，计算结果也方便存储，容易扩展；
            2. 离散后的特征对异常值更具鲁棒性，如 age>30 为 1 否则为 0，对于年龄为 200 的也不会对模型造成很大的干扰；
                3. LR 属于广义线性模型，表达能力有限，经过离散化后，每个变量有单独的权重，这相当于引入了非线性，能够提升模型的表达能力，加大拟合；
                    4. 离散后特征可以进行特征交叉，提升表达能力，由 M+N 个变量编程 M*N 个变量，进一步引入非线形，提升了表达能力；
                        5. 特征离散后模型更稳定，如用户年龄区间，不会因为用户年龄长了一岁就变化
                            6. LightGBM 在改进 XGBoost 时就增加了数据分桶，增强了模型的泛化性
  2. 此处使用 **等距数据分桶** 以 power 为例，这时候我们的缺失值也进桶了。


​    
    # bin为0-300，间距为10的等差数列,将发动机功率分成30个类别
    bin = [i*10 for i in range(31)]  
    # 用来把一组数据分割成离散的区间
    data['power_bin'] = pd.cut(data['power'], bin, labels=False)  
    # power_bin为0-29，共30个类
    data[['power_bin', 'power']].head()


最后，删除不需要的数据，axis=1表示删除一列。


​    
    data = data.drop(['creatDate', 'regDate', 'regionCode'], axis=1)
    data.columns


结果：

![](https://img-blog.csdnimg.cn/20200325214942641.png)

目前的数据其实已经可以给树模型使用了，所以我们导出一下。


​    
    data.to_csv('data_for_tree.csv', index=0)


### 特征构造（LR,NN）

不同模型对数据集的要求不同，所以分开构造 1\. 变换分布


​    
    Train_data['power'].plot.hist()


![](https://img-blog.csdnimg.cn/20200326092900961.png)

原数据集不是正态分布，我们对其取 log，在做归一化。


​    
    data['power'] = np.log(data['power'] + 1)  # +1为了让取log后的值大于0
    data['power'] = ((data['power'] - np.min(data['power'])) / (np.max(data['price']) - np.min(data['price']))) 
    data['power'].plot.hist()


![](https://img-blog.csdnimg.cn/20200326093026942.png)

  2. 归一化


​    
    def max_min(x):
        return (x - np.min(x)) / (np.max(x) - np.min(x))
    
    data['kilometer'] = max_min(data['kilometer'])
    data['brand_amount'] = max_min(data['brand_amount'])
    data['brand_price_average'] = max_min(data['brand_price_average'])
    data['brand_price_max'] = max_min(data['brand_price_max'])
    data['brand_price_median'] = max_min(data['brand_price_median'])
    data['brand_price_min'] = max_min(data['brand_price_min'])
    data['brand_price_std'] = max_min(data['brand_price_std'])
    data['brand_price_sum'] = max_min(data['brand_price_sum'])


  3. 类别特征编码

名义变量转换成哑元变量，利用pandas实现one hot
encode，可参考[网址](https://blog.csdn.net/maymay_/article/details/80198468)。


​    
    data = pd.get_dummies(data, columns=['model', 'brand', 'bodyType', 'fuelType', 'gearbox',
                                        'notRepairedDamage', 'power_bin'])
    data.columns


![](https://img-blog.csdnimg.cn/202003260939101.png)

最后，存储数据给LR用


​    
    data.to_csv('data_for_lr.csv', index=0)


### 特征筛选

  1. 过滤式

相关性分析，从相关性较大的特征之间，去除一个，可以计算出相关系数，或者看相关性矩阵图。其中，`method`参数的值可以是： \-
pearson：来衡量两个数据集合是否在一条线上面，即针对线性数据的相关系数计算，针对非线性数据便会有误差。 \-
kendall：用于反映分类变量相关性的指标，即针对无序序列的相关系数，非正太分布的数据 \- spearman：非线性的，非正太分析的数据的相关系数


​    
    print(data['power'].corr(data['price'], method='spearman'))
    print(data['kilometer'].corr(data['price'], method='spearman'))
    
    data_numeric = data[['power', 'kilometer', 'brand_amount', 'brand_price_average',
                        'brand_price_max', 'brand_price_median']]
    correlation = data_numeric.corr()
    f, ax = plt.subplots(figsize = (7, 7))
    plt.title('Correlation of Numeric Features with Price', y=1, size=16)
    sns.heatmap(correlation, square=True, vmax=0.8)


![](https://img-blog.csdnimg.cn/20200326094441960.png)

  2. 包裹式

没有详细研究，单纯记录下，使用`pip install mlxtend`安装。


​    
    from mlxtend.feature_selection import SequentialFeatureSelector as SFS
    from sklearn.linear_model import LinearRegression
    sfs = SFS(LinearRegression(),
               k_features=10,
               forward=True,
               floating=False,
               scoring = 'r2',
               cv = 0)
    x = data.drop(['price'], axis=1)
    x = x.fillna(0)
    y = data['price']
    sfs.fit(x, y)
    sfs.k_feature_names_ 


​    
​    
    # 画出来，可以看到边际效益
    from mlxtend.plotting import plot_sequential_feature_selection as plot_sfs
    import matplotlib.pyplot as plt
    fig1 = plot_sfs(sfs.get_metric_dict(), kind='std_dev')
    plt.grid()
    plt.show()


  3. 嵌入式 **大部分情况下都是使用这种方式做特征筛选！** 下一章节补上

## 经验总结

  1. 匿名特征的处理 （屯着……） 有些比赛的特征是匿名特征，这导致我们并不清楚特征相互直接的关联性，这时我们就只有单纯基于特征进行处理，比如装箱，groupby，agg 等这样一些操作进行一些特征统计，此外还可以对特征进行进一步的 log，exp 等变换，或者对多个特征进行四则运算（如上面我们算出的使用时长），多项式组合等然后进行筛选。由于特性的匿名性其实限制了很多对于特征的处理，当然有些时候用 NN 去提取一些特征也会达到意想不到的良好效果。
  2. 非匿名特征 深入分析背后的业务逻辑或者说物理原理，从而才能更好的找到 magic。
  3. 特征工程和模型结合 当然特征工程其实是和模型结合在一起的，这就是为什么要为 LR NN 做分桶和特征归一化的原因，而对于特征的处理效果和特征重要性等往往要通过模型来验证。

机器学习[基础知识](https://www.zhihu.com/people/is-aze/posts)

## 问题记录

Q1: 特征构造中为什么要把训练集和测试集放在一起？ A1：放到一起处理其中的特征，比如，时间转换，地理编码等特征构造措施。
**以及后面涉及到的统计量特征** 。 Q2：怎么理解先验知识？
A2：当你说：这是一幢别墅。你脑子里面是有“别墅”这个概念的，以及关于别墅的一些属性，然后你才知道你眼前这个东西叫做“别墅”。前面的“别墅”这个概念就是你对眼前建筑的先验知识。
**Q3：地理编码处理中，提取的部分样本的城市代码为空？【待处理】**
A3：有的二手车的regionCode为3位数，提取出的值为空，为空的样本也可以看作是同一个城市的，但是这里并没有处理，当作缺失值的吗？？？
**Q4：构造统计量特征中，test的数据是否也需要计算统计量？（很重要）**
A4：猜想：需要，训练好模型后，test数据需要有相应的统计特征，才能进行预测。 猜想不完全对。 1\.
先看下train数据的统计特征构造完后，数据融合的代码。需要注意的是，此处是将`brand_fe`的信息加入到`data`中，而此时的`data`包含test和train数据。（其中`brand_fe`是train数据`brand`特征的所有统计量）


​    
    data = data.merge(brand_fe, how='left', on='brand')
    data[['train', 'brand', 'brand_amount']]


  2. 所以便有了如下的结果，test和train数据中相同`brand`的二手车，有着相同的统计特征，比如`brand_amount、brand_min`等。但是为什么test数据的`brand`统计特征的值，要用train数据统计出的值来填充？

![](https://img-blog.csdnimg.cn/20200327172913183.png)

  3. 为了保证test和train数据的样本分布情况相同，更好的适应我们用train数据训练出的模型。如果我们对test数据的`brand`特征再进行一次统计，那统计出的特征值岂不是会随着test数据的样本情况或者样本数量的改变而改变。那为什么不对`data`数据（即test和train数据）中的`brand`特征进行统计？
  4. 那样的话，我们就需要用`data`数据来训练模型，然后再使用额外的测试数据集来测试模型。如果尝试将`data`数据再次切分成test和train数据，以此进行模型训练和预测的话，应该也可以，test和train数据的`brand`相关统计量相同。但是此时用于train的数据的统计特征与其自身实际特征不一致，效果可能不好。

Q5：数据分桶的原因没有看的太懂？ A5：

  * [x] 离散后稀疏向量内积乘法运算速度更快，计算结果也方便存储，容易扩展
  * [x] 离散后的特征对异常值更具鲁棒性，如 age>30 为 1 否则为 0，对于年龄为 200 的也不会对模型造成很大的干扰
  * [x] 特征离散后模型更稳定，如用户年龄区间，不会因为用户年龄长了一岁就变化
  * [x] 离散化后可以引入特征交叉，更好的引入非线性。如果年龄分了M个桶，收入分了N个桶，则可以组合出M*N个特征。
  * [x] 分桶之后相当于引入了一个分段函数，利用这个分段函数表达了一些非线性的意义。这也就是常说的离散化之后可以增加非线性的含义。 参考：[连续特征离散化的意义](https://zhuanlan.zhihu.com/p/66952177)

Q6：LR，NN，LightGBM ，XGBoost 是啥？ A6： \- LR：LinearRegression 线性回归 \- NN：近邻
(Nearest Neighbor) \-
XGBoost：一种改进后的树模型，详细参考[网站](https://blog.csdn.net/a1b2c3d4123456/article/details/52849091)
\-
LightGBM：微软推出了一个新的boosting框架，想要挑战xgboost的江湖地位，详细参考[网站](https://bacterous.github.io/2018/09/13/LightGBM%E4%BD%BF%E7%94%A8/)
**Q7：树模型和LR NN对数据集的要求是什么？** A7：

  1. xgb模型：无需提前处理缺失值，在迭代的过程中填补缺失值
  2. LR模型：对于线性回归模型，对y要求正态分布，类别变量要转换为哑变量

Q8：km 的比较正常，应该是已经做过分桶了，怎么看出来的？ A8：横轴为0-14的整数，应该是将汽车行驶里程数，分成了15个桶。
**Q9：长尾分布【待处理】** A9：

